# ds.py

import torch
import deepspeed
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "mistralai/Mistral-7B-v0.1"

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True,
    padding_side="left"
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# DeepSpeed inference konfiguratsiyasi
model = deepspeed.init_inference(
    model,
    mp_size=1,  # GPU soni
    dtype=torch.float16,
    replace_method='auto',
    replace_with_kernel_inject=True
)

# Sinov prompti
inputs = tokenizer("Assalomu alaykum, dunyo!", return_tensors="pt").to(device='cuda')

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=20)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
