import torch
import deepspeed
from transformers import AutoModelForCausalLM, AutoTokenizer

deepspeed.init_distributed()

model_name = "meta-llama/Llama-2-7b-hf"  # FP16 model

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)

model = deepspeed.init_inference(model,
                                 tensor_parallel={"tp_size": 2},  # yangi syntax
                                 dtype=torch.float16,
                                 replace_with_kernel_inject=True)

inputs = tokenizer("Assalomu alaykum, bugun ob-havo", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=50)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
