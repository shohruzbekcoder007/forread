# ds.py

import os
import torch
import datetime
import deepspeed
import torch.distributed as dist
from transformers import AutoTokenizer, AutoModelForCausalLM

def setup_distributed():
    local_rank = int(os.getenv('LOCAL_RANK', '0'))
    world_size = int(os.getenv('WORLD_SIZE', '2'))
    rank = int(os.getenv('RANK', '0'))
    
    # Set device
    device = f'cuda:{local_rank}'
    torch.cuda.set_device(local_rank)
    
    # Initialize process group
    torch.distributed.init_process_group(
        backend='nccl',
        init_method='env://',
        timeout=datetime.timedelta(minutes=10)
    )
    
    print(f'Process group initialized: rank={rank}, local_rank={local_rank}, '
          f'world_size={world_size}, device={device}')
    
    return local_rank, world_size, rank, device

# Initialize distributed environment
try:
    local_rank, world_size, rank, device = setup_distributed()
except Exception as e:
    print(f'Failed to initialize distributed environment: {str(e)}')
    raise e

model_name = "mistralai/Mistral-7B-v0.1"

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True,
    padding_side="left"
)

# Load model on all ranks
model = None
if rank == 0:
    print(f'Loading model on rank {rank}...')
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map=None
    ).to(device)
    print('Model loaded successfully')

# Simple barrier to sync processes
torch.distributed.barrier()

# DeepSpeed inference config
ds_config = {
    "tensor_parallel": {
        "tp_size": world_size  # Use world_size for tensor parallelism
    },
    "dtype": torch.float16,
    "replace_with_kernel_inject": True,
    "replace_method": "auto"
}

# Initialize DeepSpeed inference engine
model = deepspeed.init_inference(
    model,
    **ds_config
)

def generate_response(prompt):
    # Move inputs to correct device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        try:
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        except Exception as e:
            print(f'Error in generate: {str(e)}')
            response = 'Xatolik yuz berdi'
            
    return response

# Faqat rank 0 node input/output qiladi
if local_rank == 0:
    print("Model tayyor! Chiqish uchun 'exit' deb yozing.")
    while True:
        try:
            user_input = input("\nSiz: ")
            if user_input.lower() == 'exit':
                break
            
            response = generate_response(user_input)
            print(f"\nModel: {response}")
        except Exception as e:
            print(f"Xato yuz berdi: {e}")
            break


torchrun --nnodes=2 --nproc_per_node=1 --node_rank=1 --master_addr="172.16.8.38" --master_port=29500 ds.py
