# ds.py

import torch
import deepspeed
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "meta-llama/Llama-2-7b"

# ðŸš¨ Fast tokenizerni bloklash uchun tokenizer_class argumentidan foydalan
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=False,
    tokenizer_type="LlamaTokenizer",
    trust_remote_code=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# DeepSpeed konfiguratsiya
ds_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    use_auth_token=True,
    config={
        "train_micro_batch_size_per_gpu": 1,
        "gradient_accumulation_steps": 1,
        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": 1e-5
            }
        },
        "zero_optimization": {
            "stage": 2
        }
    }
)

inputs = tokenizer("Assalomu alaykum, dunyo!", return_tensors="pt").to(ds_engine.device)

with torch.no_grad():
    outputs = ds_engine.generate(**inputs, max_new_tokens=20)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
