# ds.py

import os
import torch
import deepspeed
import torch.distributed as dist
from transformers import AutoTokenizer, AutoModelForCausalLM

# Distributed init
torch.distributed.init_process_group(backend='nccl')
local_rank = int(os.getenv('LOCAL_RANK', '0'))
world_size = int(os.getenv('WORLD_SIZE', '2'))
torch.cuda.set_device(local_rank)

# DeepSpeed distributed init
deepspeed.init_distributed()
torch.cuda.set_device(local_rank)

model_name = "mistralai/Mistral-7B-v0.1"

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True,
    padding_side="left"
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# DeepSpeed inference config
ds_config = {
    "tensor_parallel": {
        "tp_size": 2
    },
    "dtype": torch.float16,
    "replace_with_kernel_inject": True
}

# Initialize DeepSpeed inference engine
model = deepspeed.init_inference(
    model,
    **ds_config
)

def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device='cuda')
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,  # Uzunroq javob
            temperature=0.7,     # Ijodkorlik darajasi
            top_p=0.9,          # Sampling uchun
            do_sample=True,      # Har safar turli javob
            pad_token_id=tokenizer.eos_token_id
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Faqat rank 0 node input/output qiladi
if local_rank == 0:
    print("Model tayyor! Chiqish uchun 'exit' deb yozing.")
    while True:
        try:
            user_input = input("\nSiz: ")
            if user_input.lower() == 'exit':
                break
            
            response = generate_response(user_input)
            print(f"\nModel: {response}")
        except Exception as e:
            print(f"Xato yuz berdi: {e}")
            break
