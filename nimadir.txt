# ds.py

import os
import torch
import deepspeed
import torch.distributed as dist
from transformers import AutoTokenizer, AutoModelForCausalLM

# Get environment variables
local_rank = int(os.getenv('LOCAL_RANK', '0'))
world_size = int(os.getenv('WORLD_SIZE', '2'))
rank = int(os.getenv('RANK', '0'))

# Set cuda device
torch.cuda.set_device(local_rank)

# Initialize process group
try:
    if not torch.distributed.is_initialized():
        torch.distributed.init_process_group(
            backend='nccl',
            init_method='env://'
        )
        print(f'Initialized process group: rank={rank}, world_size={world_size}')
except Exception as e:
    print(f'Failed to initialize process group: {str(e)}')
    raise e
torch.cuda.set_device(local_rank)

model_name = "mistralai/Mistral-7B-v0.1"

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True,
    padding_side="left"
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# DeepSpeed inference config
ds_config = {
    "tensor_parallel": {
        "tp_size": 2
    },
    "dtype": torch.float16,
    "replace_with_kernel_inject": True
}

# Initialize DeepSpeed inference engine
model = deepspeed.init_inference(
    model,
    **ds_config
)

def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device='cuda')
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,  # Uzunroq javob
            temperature=0.7,     # Ijodkorlik darajasi
            top_p=0.9,          # Sampling uchun
            do_sample=True,      # Har safar turli javob
            pad_token_id=tokenizer.eos_token_id
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Faqat rank 0 node input/output qiladi
if local_rank == 0:
    print("Model tayyor! Chiqish uchun 'exit' deb yozing.")
    while True:
        try:
            user_input = input("\nSiz: ")
            if user_input.lower() == 'exit':
                break
            
            response = generate_response(user_input)
            print(f"\nModel: {response}")
        except Exception as e:
            print(f"Xato yuz berdi: {e}")
            break
