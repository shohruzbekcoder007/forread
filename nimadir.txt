# ds.py

import torch
import deepspeed
from transformers import AutoTokenizer, AutoModelForCausalLM

# ðŸš€ 1. Model nomi
model_name = "meta-llama/Llama-2-7b"

# ðŸš€ 2. Tokenizer yuklash (SentencePiece uchun use_fast=False)
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=False,
    local_files_only=False,  # cachedan emas, huggingface'dan tekshiradi
    trust_remote_code=True
)

# ðŸš€ 3. Model yuklash
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# ðŸš€ 4. DeepSpeed konfiguratsiya
ds_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    config={
        "train_micro_batch_size_per_gpu": 1,
        "gradient_accumulation_steps": 1,
        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": 1e-5
            }
        },
        "zero_optimization": {
            "stage": 2
        }
    }
)

# ðŸš€ 5. Sinov input
inputs = tokenizer("Assalomu alaykum, dunyo!", return_tensors="pt").to(ds_engine.device)

# ðŸš€ 6. Forward pass
with torch.no_grad():
    outputs = ds_engine.generate(**inputs, max_new_tokens=20)

# ðŸš€ 7. Natija
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
