# ds.py

import os
import torch
import deepspeed
import torch.distributed as dist
from transformers import AutoTokenizer, AutoModelForCausalLM

# Get environment variables
local_rank = int(os.getenv('LOCAL_RANK', '0'))
world_size = int(os.getenv('WORLD_SIZE', '2'))
rank = int(os.getenv('RANK', '0'))
master_addr = os.getenv('MASTER_ADDR', '172.16.8.38')
master_port = os.getenv('MASTER_PORT', '29500')

# Set cuda device first
torch.cuda.set_device(local_rank)

# Initialize process group with explicit settings
try:
    if not torch.distributed.is_initialized():
        init_method = f'tcp://{master_addr}:{master_port}'
        torch.distributed.init_process_group(
            backend='nccl',
            init_method=init_method,
            world_size=world_size,
            rank=rank
        )
        print(f'Initialized process group: rank={rank}, world_size={world_size}, init_method={init_method}')
except Exception as e:
    print(f'Failed to initialize process group: {str(e)}')
    raise e

model_name = "mistralai/Mistral-7B-v0.1"

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True,
    padding_side="left"
)

# Load model on rank 0 first
if rank == 0:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map=None  # Don't use auto device mapping
    )
    # Move model to GPU
    model = model.to(f'cuda:{local_rank}')
else:
    model = None

# Make sure all processes are ready before proceeding
torch.distributed.barrier(device_ids=[local_rank])

# DeepSpeed inference config
ds_config = {
    "tensor_parallel": {
        "tp_size": world_size  # Use world_size for tensor parallelism
    },
    "dtype": torch.float16,
    "replace_with_kernel_inject": True,
    "replace_method": "auto"
}

# Initialize DeepSpeed inference engine
model = deepspeed.init_inference(
    model,
    **ds_config
)

def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device='cuda')
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,  # Uzunroq javob
            temperature=0.7,     # Ijodkorlik darajasi
            top_p=0.9,          # Sampling uchun
            do_sample=True,      # Har safar turli javob
            pad_token_id=tokenizer.eos_token_id
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Faqat rank 0 node input/output qiladi
if local_rank == 0:
    print("Model tayyor! Chiqish uchun 'exit' deb yozing.")
    while True:
        try:
            user_input = input("\nSiz: ")
            if user_input.lower() == 'exit':
                break
            
            response = generate_response(user_input)
            print(f"\nModel: {response}")
        except Exception as e:
            print(f"Xato yuz berdi: {e}")
            break


torchrun --nnodes=2 --nproc_per_node=1 --node_rank=1 --master_addr="172.16.8.38" --master_port=29500 ds.py